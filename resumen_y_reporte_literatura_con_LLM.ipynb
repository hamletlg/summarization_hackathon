{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema número 5: Summarizing with LLM ##"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Planteo del problema: ### \n",
    "\n",
    "Se propone la creación de un agente que reciba un archivo Bibtex como entrada, y devuelva un informe de texto con un resumen de los documentos. Además que mencione documentos, investigadores e instituciones mas relevantes relacionados con la temática de los documentos resumidos.\n",
    "\n",
    "### Abordaje del problema: ###\n",
    "\n",
    "Se entendió el problema como una secuencia lineal de pasos:\n",
    "1. Entrada de fichero bibtex\n",
    "2. Procesamiento de fichero y extraccion de titulos\n",
    "3. Descarga de documentos correspondientes a cada titulo\n",
    "4. Resumir cada documento\n",
    "5. Obtener topico de cada documento\n",
    "6. Por cada topico obtener articulos, autores e instituciones mas relevantes relacionados con el topico\n",
    "7. Reportar\n",
    "\n",
    "### Decisiones tomadas para la solución del problema: ###\n",
    "\n",
    "1. Se trabajará con documentos pdf. Una solución más completa deberá considerar otras alternativas.\n",
    "\n",
    "2. En el planteo del problema se habla de \"temas\". Estos se van a entender más bien como tópicos. Y cada documento tendrá un solo tópico. Esto facilita la formulación de la búsqueda en Google Scholar. Una solución más completa deberá considerar documentos con varios tópicos, como las compilaciones de articulos. \n",
    "\n",
    "3. La fuente de autoridad para recuperar autores, articulos e instituciones mas relevantes será el ranking de Google Scholar. Esto implica que los que firman los artículos más relevantes para un tópico determinado serán también los autores más relevantes, y lo mismo ocurre con sus instituciones de pertenencia. Esta es una simplificación que en dependencia de la disciplina puede ser excesiva. Una solución más completa deberá aplicar estudios de reputación, análisis de comunidades de práctica, entre otras metodologías de la cienciometría. Además en dependencia de la disciplina se deberá considerar otros tipos de instituciones, por ejemplo regulatorias, que sin dejar de ser relevantes y con impacto en un campo no producen publicaciones académicas.\n",
    "\n",
    "4. El reporte final será texto plano, sin formato. En una solución mas completa deberá considerarse el uso de plantillas para formatear el texto y potenciar la legibilidad del reporte.\n",
    "\n",
    "5. Se usará la librería LangChain. Esta librería se convirtió en la fuente de aprendizaje para conocer sobre agentes y chains, además de la caja de herramientas principal. Esto conllevó a la decisión más dificil y la que tomó más tiempo asumir en el ejercicio: No se va a codificar un agente, en sentido estricto y como lo entiende LangChain. Un agente según los autores de la librería tiene un flujo de trabajo no lineal, flexible, donde el agente determina el curso de acción, y elige cuáles herramienta usar entre las que tiene a su disposición. Al entenderse el problema como un proceso lineal ya no se acomoda al flujo de trabajo de un agente, a pesar de producir las salidas deseadas.\n",
    "\n",
    "6. Una solución más completa deberá construir un agente con una serie de herramientas a su disposición que le permita planificar adecuadamente la secuencia de acciones y cambiar dinámicamente hacia flujos alternativos de trabajo. Por ejemplo, refinar iterativamente resultados de búsqueda en Google Scholar, adaptar las estrategias para hallar autores, articulos e instituciones mas relevantes a la disciplina de la que se trate, ser más flexible en el procesamiento de la información y sus variados soportes, formatear adecuadamente el reporte al usuario, entre otras funcionalidades. \n",
    "\n",
    "### Estructura del notebook ###\n",
    "\n",
    "El notebook desarrolla el abordaje planteado del problema como una secuencia lineal de pasos, y  está estructurado en 6 celdas. Además de las usuales de importar las librerías necesarias y configurar variables iniciales, el resto de las celdas son:\n",
    "- Funciones para trabajar con Bibtex\n",
    "- Funciones para búsqueda de informacion en Google\n",
    "- Uso de LangChain\n",
    "- Ciclo principal de ejecución\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importa las librerias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "from bibtexparser import loads\n",
    "from serpapi import GoogleSearch\n",
    "from langchain import OpenAI, PromptTemplate, LLMChain\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configura variables de entorno y otras variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llaves\n",
    "serpapi_key = ''\n",
    "os.environ['OPENAI_API_KEY'] = ''\n",
    "\n",
    "# Rutas\n",
    "path_bibtex_file = \"test-data/items2.bib\"\n",
    "path_download_files = 'downloaded_texts'\n",
    "\n",
    "# Modelo a usar y parametros fundamentales. \n",
    "# Se especifica temperatura = 0 para asegurar repetibilidad de resultados\n",
    "# y se mantiene modelo definido por defecto en LangChain\n",
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones para trabajar con los ficheros Bibtex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsea fichero Bibtex y devuelve lista de titulos\n",
    "def load_bibtex(file_path):\n",
    "  with open(file_path) as bibtex_file:\n",
    "    bib_db = loads(bibtex_file.read())\n",
    "  titles = []\n",
    "  for entry in bib_db.entries:    \n",
    "    title = entry.get(\"title\", \"\")    \n",
    "    titles.append(title)\n",
    "  return titles\n",
    "\n",
    "# Devuelve url de descarga del pdf a partir de titulo de documento\n",
    "def get_url_from_title(title):\n",
    "    url = None\n",
    "    search_params = {\n",
    "        \"engine\": \"google_scholar\",\n",
    "        \"q\": f\"{title}\",\n",
    "        \"api_key\": serpapi_key,        \n",
    "        \"hl\": \"en\"\n",
    "    }\n",
    "    search = GoogleSearch(search_params)\n",
    "    results = search.get_dict()\n",
    "    if 'resources' in results['organic_results'][0]:\n",
    "        if 'PDF' in str(results['organic_results'][0]['resources'][0]):\n",
    "            url = results['organic_results'][0]['resources'][0]['link']\n",
    "    return url\n",
    "\n",
    "# Descarga documento a partir de url y devuelve ruta de documento \n",
    "def download_document(url, folder, title):\n",
    "  # Limpia y acorta string title\n",
    "  title = re.sub('[^a-zA-Z]', '', title)\n",
    "  title = title[:12]\n",
    "  file_name = title + '.pdf'\n",
    "  # Verifica existencia de carpeta antes de descarga\n",
    "  if not os.path.exists(folder):\n",
    "      os.makedirs(folder)\n",
    "  file_path = os.path.join(folder, file_name)\n",
    "  session = requests.Session()\n",
    "  response = session.get(url, stream=True, headers={'User-Agent': 'Mozilla/5.0'}, allow_redirects=True)\n",
    "  # Si request fue exitoso (status code 200),\n",
    "  # descarga fichero\n",
    "  if response.status_code == 200:\n",
    "      print('Downloading...')  \n",
    "      with open(file_path, 'wb') as f:\n",
    "           for chunk in response.iter_content(4096):\n",
    "            f.write(chunk)\n",
    "      if os.path.isfile(file_path):\n",
    "            print('File downloaded successfully to '+ file_path + ' !')\n",
    "      else:\n",
    "            print('Download failed.')\n",
    "            return None\n",
    "      return file_path\n",
    "  else:\n",
    "      print('Download failed.')\n",
    "      return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones para búsqueda de informacion bibliografica usando SerpAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devuelve lista de ids de autores, y cadena con titulos y autores\n",
    "# a partir de topico\n",
    "def get_ids_authors_articles(topic):\n",
    "    res = ''\n",
    "    authors_ids = []\n",
    "    search_params = {\n",
    "        \"engine\": \"google_scholar\",\n",
    "        \"q\": f\"{topic}\",\n",
    "        \"api_key\": serpapi_key,\n",
    "        \"num\": 10,\n",
    "        \"hl\": \"en\"\n",
    "    }\n",
    "    search = GoogleSearch(search_params)\n",
    "    results = search.get_dict()\n",
    "    results_organic = results[\"organic_results\"]\n",
    "    # Extrae la informacion de cada articulo\n",
    "    for result in results_organic:\n",
    "        title = result[\"title\"]\n",
    "        publication_info = result[\"publication_info\"]\n",
    "        if 'authors' in publication_info:\n",
    "            authors = [author[\"name\"] for author in publication_info[\"authors\"]]\n",
    "            first_author_id = result['publication_info']['authors'][0]['author_id']    \n",
    "            res += \"Title: \" + title + '/n'\n",
    "            res += \"Authors: \" + str(authors) + '/n'\n",
    "            authors_ids.append(first_author_id)        \n",
    "    return authors_ids, res\n",
    "\n",
    "\n",
    "# Devuelve dict autor:afiliacion a partir lista de ids de autores\n",
    "def get_author_affiliations(author_ids):\n",
    "    affiliations = {}\n",
    "    for author_id in author_ids:\n",
    "        url = f\"https://serpapi.com/search.json?engine=google_scholar_author&author_id={author_id}&api_key={serpapi_key}\"\n",
    "        response = requests.get(url)\n",
    "        data = json.loads(response.text)\n",
    "        if 'affiliations' in data[\"author\"]:\n",
    "            affiliations[author_id] = data[\"author\"][\"affiliations\"]\n",
    "        else:\n",
    "            affiliations[author_id] = None\n",
    "    return affiliations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones para resumir documentos, extraer topicos y reportar sobre autores, documentos e instituciones principales por topico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devuelve resumen de documento situado en document_path \n",
    "# usando LLMChain\n",
    "def summarize_document(document_path):\n",
    "    loader = PyMuPDFLoader(document_path)\n",
    "    docs = loader.load() \n",
    "    text = '' \n",
    "    # Chequea si el pdf devuelve un string usable\n",
    "    for doc in docs:\n",
    "        text += doc.page_content\n",
    "    pattern = r'[a-zA-Z]'\n",
    "    matches = re.findall(pattern, text)\n",
    "    if bool(matches):\n",
    "        chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "        summary = chain.run(docs)\n",
    "        return summary\n",
    "    return None \n",
    "\n",
    "# Devuelve topico a partir de resumen usando LLMChain\n",
    "def extract_topic(summary):\n",
    "    prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"From the abstract in this text extract its main topic: {text}\",\n",
    ")\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    topic = chain.run(summary)\n",
    "    return topic\n",
    "\n",
    "# Devuelve 5 principales autores a partir de cadena con informacion\n",
    "# de autores y articulos usando LLMChain, para manejar adecuadamente \n",
    "# ambiguedades, en los nombres, repeticiones, etc\n",
    "def extract_main_authors(authors_and_articles):\n",
    "    prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"From the following text extract the top five authors. Each author in a new line: {text}\",\n",
    ")\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    main_authors = chain.run(authors_and_articles)\n",
    "    return main_authors\n",
    "\n",
    "# Devuelve 5 principales articulos a partir de cadena con informacion\n",
    "# de autores y articulos usando LLMChain, para manejar adecuadamente\n",
    "# titulos repetidos, con pequeñas variaciones, etc\n",
    "def extract_main_articles(authors_and_articles):\n",
    "    prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"From the following text extract the top five titles. Mention its authors: {text}\",\n",
    ")\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    main_articles = chain.run(authors_and_articles)\n",
    "    return main_articles\n",
    "\n",
    "# Devuelve principales centros e instituciones para determinado \n",
    "# topico a partir de las afiliaciones de autores\n",
    "def report_main_centers_and_institutions(affiliations):\n",
    "    prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"Report only institution names from the following text. Ignore personal names, titles and roles. Also ignore incomplete information. Each institution in a new line: {text}\",\n",
    ")\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    top_centers = chain.run(str(affiliations))\n",
    "    return top_centers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ciclo principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devuelve lista de titulos a partir de fichero Bibtex en path_bibtex_file\n",
    "titles = load_bibtex(path_bibtex_file)\n",
    "\n",
    "for title in titles:\n",
    "    # Obten url de descarga de version pdf del titulo\n",
    "    url = get_url_from_title(title)\n",
    "\n",
    "    # Si no se pudo obtener version pdf del documento pasa al siguiente titulo\n",
    "    if url == None:\n",
    "        print('Impossible to find pdf of: ' + title)\n",
    "        continue\n",
    "    \n",
    "    # Descarga el documento\n",
    "    document_path = download_document(url, path_download_files, title)\n",
    "\n",
    "    # Si no se pudo descargar el documento pasa al siguiente titulo\n",
    "    if document_path == None:\n",
    "        print('Impossible to download pdf of: ' + title)\n",
    "        continue\n",
    "    \n",
    "    # Resume el documento\n",
    "    summary = summarize_document(document_path)\n",
    "\n",
    "    # Si no se pudo extraer texto del documento pasa al siguiente titulo\n",
    "    if summary == None:\n",
    "        print('Impossible to extract text from pdf: ' + title)\n",
    "        continue\n",
    "    \n",
    "    # Extrae el topico del documento\n",
    "    topic = extract_topic(summary)\n",
    "    \n",
    "    # Obten la informacion sobre principales autores y articulos en\n",
    "    # el topico\n",
    "    author_ids, authors_and_articles = get_ids_authors_articles(topic)\n",
    "\n",
    "    # Obten principales autores\n",
    "    main_authors = extract_main_authors(authors_and_articles) \n",
    "\n",
    "    # Obten principales articulos\n",
    "    main_articles = extract_main_articles(authors_and_articles) \n",
    "    \n",
    "    # Obten afiliaciones de cada autor\n",
    "    affiliations = get_author_affiliations(author_ids)\n",
    "    \n",
    "    # Extrae principales centros e instituciones de las afiliaciones\n",
    "    top_centers = report_main_centers_and_institutions(str(affiliations))\n",
    "    \n",
    "    # Imprime la informacion obtenida para este titulo\n",
    "    print('')\n",
    "    print(f'Title: {title}')\n",
    "    print('Abstract:')\n",
    "    print(summary)\n",
    "    print(topic)\n",
    "    print('Main_authors in the topic: ')\n",
    "    print(main_authors)\n",
    "    print('')\n",
    "    print('Main documents in the topic: ')\n",
    "    print(main_articles)\n",
    "    print('')\n",
    "    print('Main centers and institutions: ')\n",
    "    print(top_centers)\n",
    "    print('')\n",
    "    print('--------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-llama-cpp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
