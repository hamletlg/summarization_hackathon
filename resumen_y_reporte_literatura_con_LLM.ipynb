{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Problema número 5: Summarizing with LLM ##"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Planteo del problema: ### \n",
        "\n",
        "Se propone la creación de un agente que reciba un archivo Bibtex como entrada, y devuelva un informe de texto con un resumen de los documentos. Además que mencione documentos, investigadores e instituciones mas relevantes relacionados con la temática de los documentos resumidos.\n",
        "\n",
        "### Abordaje del problema: ###\n",
        "\n",
        "Se entendió el problema como una secuencia lineal de pasos:\n",
        "1. Entrada de fichero bibtex\n",
        "2. Procesamiento de fichero y extraccion de titulos\n",
        "3. Descarga de documentos correspondientes a cada titulo\n",
        "4. Resumir cada documento\n",
        "5. Obtener topico de cada documento\n",
        "6. Por cada topico obtener articulos, autores e instituciones mas relevantes relacionados con el topico\n",
        "7. Reportar\n",
        "\n",
        "### Decisiones tomadas para la solución del problema: ###\n",
        "\n",
        "1. Se trabajará con documentos pdf. Una solución más completa deberá considerar otras alternativas.\n",
        "\n",
        "2. En el planteo del problema se habla de \"temas\". Estos se van a entender más bien como tópicos. Y cada documento tendrá un solo tópico. Esto facilita la formulación de la búsqueda en Google Scholar. Una solución más completa deberá considerar documentos con varios tópicos, como las compilaciones de articulos. \n",
        "\n",
        "3. La fuente de autoridad para recuperar autores, articulos e instituciones mas relevantes será el ranking de Google Scholar. Esto implica que los que firman los artículos más relevantes para un tópico determinado serán también los autores más relevantes, y lo mismo ocurre con sus instituciones de pertenencia. Esta es una simplificación que en dependencia de la disciplina puede ser excesiva. Una solución más completa deberá aplicar estudios de reputación, análisis de comunidades de práctica, entre otras metodologías de la cienciometría. Además en dependencia de la disciplina se deberá considerar otros tipos de instituciones, por ejemplo regulatorias, que sin dejar de ser relevantes y con impacto en un campo no producen publicaciones académicas.\n",
        "\n",
        "4. El reporte final será texto plano, sin formato. En una solución mas completa deberá considerarse el uso de plantillas para formatear el texto y potenciar la legibilidad del reporte.\n",
        "\n",
        "5. Se usará la librería LangChain. Esta librería se convirtió en la fuente de aprendizaje para conocer sobre agentes y chains, además de la caja de herramientas principal. Esto conllevó a la decisión más dificil y la que tomó más tiempo asumir en el ejercicio: No se va a codificar un agente, en sentido estricto y como lo entiende LangChain. Un agente según los autores de la librería tiene un flujo de trabajo no lineal, flexible, donde el agente determina el curso de acción, y elige cuáles herramienta usar entre las que tiene a su disposición. Al entenderse el problema como un proceso lineal ya no se acomoda al flujo de trabajo de un agente, a pesar de producir las salidas deseadas.\n",
        "\n",
        "6. Una solución más completa deberá construir un agente con una serie de herramientas a su disposición que le permita planificar adecuadamente la secuencia de acciones y cambiar dinámicamente hacia flujos alternativos de trabajo. Por ejemplo, refinar iterativamente resultados de búsqueda en Google Scholar, adaptar las estrategias para hallar autores, articulos e instituciones mas relevantes a la disciplina de la que se trate, ser más flexible en el procesamiento de la información y sus variados soportes, formatear adecuadamente el reporte al usuario, entre otras funcionalidades. \n",
        "\n",
        "### Estructura del notebook ###\n",
        "\n",
        "El notebook desarrolla el abordaje planteado del problema como una secuencia lineal de pasos, y  está estructurado en 6 celdas. Además de las usuales de importar las librerías necesarias y configurar variables iniciales, el resto de las celdas son:\n",
        "- Funciones para trabajar con Bibtex\n",
        "- Funciones para búsqueda de informacion en Google\n",
        "- Uso de LangChain\n",
        "- Ciclo principal de ejecución\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importa las librerias necesarias"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import requests\n",
        "from bibtexparser import loads\n",
        "from serpapi import GoogleSearch\n",
        "from langchain import OpenAI, PromptTemplate, LLMChain\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain.chains.summarize import load_summarize_chain"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configura variables de entorno y otras variables"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Llaves\n",
        "serpapi_key = '87a86e2ab891417249664972be37945acbad4b53ade548f02421201c8971aab7'\n",
        "os.environ['OPENAI_API_KEY'] = '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'\n",
        "\n",
        "# Rutas\n",
        "path_bibtex_file = \"test-data/items2.bib\"\n",
        "path_download_files = 'downloaded_texts'\n",
        "\n",
        "# Modelo a usar y parametros fundamentales\n",
        "llm = OpenAI(temperature=0)"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funciones para trabajar con los ficheros Bibtex"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Parsea fichero Bibtex y devuelve lista de titulos\n",
        "def load_bibtex(file_path):\n",
        "  with open(file_path) as bibtex_file:\n",
        "    bib_db = loads(bibtex_file.read())\n",
        "  titles = []\n",
        "  for entry in bib_db.entries:    \n",
        "    title = entry.get(\"title\", \"\")    \n",
        "    titles.append(title)\n",
        "  return titles\n",
        "\n",
        "# Devuelve url de descarga del pdf a partir de titulo de documento\n",
        "def get_url_from_title(title):\n",
        "    url = None\n",
        "    search_params = {\n",
        "        \"engine\": \"google_scholar\",\n",
        "        \"q\": f\"{title}\",\n",
        "        \"api_key\": serpapi_key,        \n",
        "        \"hl\": \"en\"\n",
        "    }\n",
        "    search = GoogleSearch(search_params)\n",
        "    results = search.get_dict()\n",
        "    if 'resources' in results['organic_results'][0]:\n",
        "        if 'PDF' in str(results['organic_results'][0]['resources'][0]):\n",
        "            url = results['organic_results'][0]['resources'][0]['link']\n",
        "    return url\n",
        "\n",
        "# Descarga documento a partir de url y devuelve ruta de documento \n",
        "def download_document(url, folder, title):\n",
        "  # Limpia y acorta string title\n",
        "  title = re.sub('[^a-zA-Z]', '', title)\n",
        "  title = title[:12]\n",
        "  file_name = title + '.pdf'\n",
        "  # Verifica existencia de carpeta antes de descarga\n",
        "  if not os.path.exists(folder):\n",
        "      os.makedirs(folder)\n",
        "  file_path = os.path.join(folder, file_name)\n",
        "  session = requests.Session()\n",
        "  response = session.get(url, stream=True, headers={'User-Agent': 'Mozilla/5.0'}, allow_redirects=True)\n",
        "  # Si request fue exitoso (status code 200),\n",
        "  # descarga fichero\n",
        "  if response.status_code == 200:\n",
        "      print('Downloading...')  \n",
        "      with open(file_path, 'wb') as f:\n",
        "           for chunk in response.iter_content(4096):\n",
        "            f.write(chunk)\n",
        "      if os.path.isfile(file_path):\n",
        "            print('File downloaded successfully to '+ file_path + ' !')\n",
        "      else:\n",
        "            print('Download failed.')\n",
        "            return None\n",
        "      return file_path\n",
        "  else:\n",
        "      print('Download failed.')\n",
        "      return None"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funciones para procesar informacion bibliografica usando SerpAPI"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Devuelve lista de ids de autores, y cadena con titulos y autores\n",
        "# a partir de topico\n",
        "def get_ids_authors_articles(topic):\n",
        "    res = ''\n",
        "    authors_ids = []\n",
        "    search_params = {\n",
        "        \"engine\": \"google_scholar\",\n",
        "        \"q\": f\"{topic}\",\n",
        "        \"api_key\": serpapi_key,\n",
        "        \"num\": 10,\n",
        "        \"hl\": \"en\"\n",
        "    }\n",
        "    search = GoogleSearch(search_params)\n",
        "    results = search.get_dict()\n",
        "    results_organic = results[\"organic_results\"]\n",
        "    # Extrae la informacion de cada articulo\n",
        "    for result in results_organic:\n",
        "        title = result[\"title\"]\n",
        "        publication_info = result[\"publication_info\"]\n",
        "        if 'authors' in publication_info:\n",
        "            authors = [author[\"name\"] for author in publication_info[\"authors\"]]\n",
        "            first_author_id = result['publication_info']['authors'][0]['author_id']    \n",
        "            res += \"Title: \" + title + '/n'\n",
        "            res += \"Authors: \" + str(authors) + '/n'\n",
        "            authors_ids.append(first_author_id)        \n",
        "    return authors_ids, res\n",
        "\n",
        "\n",
        "# Devuelve dict autor:afiliacion a partir lista de ids de autores\n",
        "def get_author_affiliations(author_ids):\n",
        "    affiliations = {}\n",
        "    for author_id in author_ids:\n",
        "        url = f\"https://serpapi.com/search.json?engine=google_scholar_author&author_id={author_id}&api_key={serpapi_key}\"\n",
        "        response = requests.get(url)\n",
        "        data = json.loads(response.text)\n",
        "        if 'affiliations' in data[\"author\"]:\n",
        "            affiliations[author_id] = data[\"author\"][\"affiliations\"]\n",
        "        else:\n",
        "            affiliations[author_id] = None\n",
        "    return affiliations"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funciones para resumir documentos, extraer topicos y reportar sobre autores, documentos e instituciones principales por topico"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Devuelve resumen de documento situado en document_path \n",
        "# usando LLMChain\n",
        "def summarize_document(document_path):\n",
        "    loader = PyMuPDFLoader(document_path)\n",
        "    docs = loader.load() \n",
        "    text = '' \n",
        "    # Chequea si el pdf devuelve un string usable\n",
        "    for doc in docs:\n",
        "        text += doc.page_content\n",
        "    pattern = r'[a-zA-Z]'\n",
        "    matches = re.findall(pattern, text)\n",
        "    if bool(matches):\n",
        "        chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
        "        summary = chain.run(docs)\n",
        "        return summary\n",
        "    return None \n",
        "\n",
        "# Devuelve topico a partir de resumen usando LLMChain\n",
        "def extract_topic(summary):\n",
        "    prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=\"From the abstract in this text extract its main topic: {text}\",\n",
        ")\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    topic = chain.run(summary)\n",
        "    return topic\n",
        "\n",
        "# Devuelve principales autores a partir de cadena con informacion\n",
        "# de autores y articulos usando LLMChain\n",
        "def extract_main_authors(authors_and_articles):\n",
        "    prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=\"From the following text extract the top five authors. Each author in a new line: {text}\",\n",
        ")\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    main_authors = chain.run(authors_and_articles)\n",
        "    return main_authors\n",
        "\n",
        "# Devuelve principales articulos a partir de cadena con informacion\n",
        "# de autores y articulos usando LLMChain\n",
        "def extract_main_articles(authors_and_articles):\n",
        "    prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=\"From the following text extract the top five titles. Mention its authors: {text}\",\n",
        ")\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    main_articles = chain.run(authors_and_articles)\n",
        "    return main_articles\n",
        "\n",
        "# Devuelve principales centros e instituciones para determinado \n",
        "# topico a partir de las afiliaciones de autores\n",
        "def report_main_centers_and_institutions(affiliations):\n",
        "    prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=\"Report only institution names from the following text. Ignore personal names, titles and roles. Also ignore incomplete information. Each institution in a new line: {text}\",\n",
        ")\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    top_centers = chain.run(str(affiliations))\n",
        "    return top_centers"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ciclo principal"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Devuelve lista de titulos a partir de fichero Bibtex en path_bibtex_file\n",
        "titles = load_bibtex(path_bibtex_file)\n",
        "\n",
        "for title in titles:\n",
        "    # Obten url de descarga de version pdf del titulo\n",
        "    url = get_url_from_title(title)\n",
        "\n",
        "    # Si no se pudo obtener version pdf del documento pasa al siguiente titulo\n",
        "    if url == None:\n",
        "        print('Impossible to find pdf of: ' + title)\n",
        "        continue\n",
        "    \n",
        "    # Descarga el documento\n",
        "    document_path = download_document(url, path_download_files, title)\n",
        "\n",
        "    # Si no se pudo descargar el documento pasa al siguiente titulo\n",
        "    if document_path == None:\n",
        "        print('Impossible to download pdf of: ' + title)\n",
        "        continue\n",
        "    \n",
        "    # Resume el documento\n",
        "    summary = summarize_document(document_path)\n",
        "\n",
        "    # Si no se pudo extraer texto del documento pasa al siguiente titulo\n",
        "    if summary == None:\n",
        "        print('Impossible to extract text from pdf: ' + title)\n",
        "        continue\n",
        "    \n",
        "    # Extrae el topico del documento\n",
        "    topic = extract_topic(summary)\n",
        "    \n",
        "    # Obten la informacion sobre principales autores y articulos en\n",
        "    #  el topico\n",
        "    author_ids, authors_and_articles = get_ids_authors_articles(topic)\n",
        "\n",
        "    # Obten principales autores\n",
        "    main_authors = extract_main_authors(authors_and_articles) \n",
        "\n",
        "    # Obten principales articulos\n",
        "    main_articles = extract_main_articles(authors_and_articles) \n",
        "    \n",
        "    # Obten afiliaciones de cada autor\n",
        "    affiliations = get_author_affiliations(author_ids)\n",
        "    \n",
        "    # Extrae principales centros e instituciones de las afiliaciones\n",
        "    top_centers = report_main_centers_and_institutions(str(affiliations))\n",
        "    \n",
        "    # Imprime la informacion obtenida para este titulo\n",
        "    print()\n",
        "    print(f'Title: {title}')\n",
        "    print('Abstract:')\n",
        "    print(summary)\n",
        "    print(topic)\n",
        "    print('Main_authors in the topic: ')\n",
        "    print(main_authors)\n",
        "    print('')\n",
        "    print('Main documents in the topic: ')\n",
        "    print(main_articles)\n",
        "    print('')\n",
        "    print('Main centers and institutions: ')\n",
        "    print(top_centers)\n",
        "    print()\n",
        "    print('--------------------------------------')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descargando...\n",
            "Fichero descargado exitosamente en downloaded_texts/TheCOVIDvacc.pdf !\n",
            "Impossible to extract text from pdf: The COVID-19 vaccine development landscape\n",
            "Descargando...\n",
            "Fichero descargado exitosamente en downloaded_texts/SputnikVCOVI.pdf !\n",
            "Title: Sputnik V COVID-19 vaccine candidate appears safe and effective\n",
            "Abstract:\n",
            " The Sputnik V COVID-19 vaccine, which uses a heterologous recombinant adenovirus approach with adenovirus 26 (Ad26) and adenovirus 5 (Ad5) as vectors for the expression of the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) spike protein, has been found to be 91.6% effective in preventing COVID-19. The vaccine has been criticized for its haste and lack of transparency, but the results demonstrate the scientific principle of vaccination. Three fatalities occurred in the vaccine group, but were deemed unrelated to the vaccine.\n",
            "\n",
            "\n",
            "Main Topic: The Sputnik V COVID-19 Vaccine\n",
            "Main_authors in the topic: \n",
            "\n",
            "\n",
            "F Babamahmoodi\n",
            "M Saeedi\n",
            "R Alizadeh-Navaei\n",
            "A Amirkafi\n",
            "A Tehrani-Banihashemi\n",
            "\n",
            "Main documents in the topic: \n",
            "\n",
            "\n",
            "1. Side effects and Immunogenicity following administration of the Sputnik V COVID-19 vaccine in health care workers in Iran - Authors: F Babamahmoodi, M Saeedi, R Alizadeh-Navaei\n",
            "2. Adverse effects following COVID-19 vaccination in Iran - Authors: A Amirkafi, A Tehrani-Banihashemi\n",
            "3. Controversy surrounding the Sputnik V vaccine - Authors: P Rogliani, F Mazzeo, MG Matera\n",
            "4. sputnik-v induced spike protein antibody levels in Pakistan: an edge of sputnik-V over sinopharm and SinoVac as commercially available COVID-19 Vaccines - Authors: U Saeed, R Uppal\n",
            "5. The impact of COVID-19 vaccination programme in the Republic of San Marino: Focus on effectiveness of Gam-Covid-Vac - Authors: P Piselli\n",
            "\n",
            "Main centers and institutions: \n",
            "\n",
            "\n",
            "Mazandaran University of medical sciences\n",
            "Iran University of Medical Sciences\n",
            "Università di Roma \"Tor Vergata\"\n",
            "AJOU University School of Medicine Suwon South Korea\n",
            "INMI \"L. Spallanzani\" IRCCS\n",
            "University of Ghana\n",
            "ICMR-National Institute of Virology\n",
            "NHS Grampian & University of Aberdeen\n",
            "\n",
            "--------------------------------------\n",
            "Descargando...\n",
            "Fichero descargado exitosamente en downloaded_texts/Safetyandimm.pdf !\n",
            "Title: Safety and immunogenicity of the Ad26. COV2. S COVID-19 vaccine candidate: interim results of a phase 1/2a, double-blind, randomized, placebo-controlled trial\n",
            "Abstract:\n",
            " This preprint examines the safety, reactogenicity, and immunogenicity of Ad26.COV2.S, a potential vaccine against COVID-19. Results showed robust Th1 and CD8+ T cell responses in both cohorts, with no or very low Th2 responses, indicating a Th1-skewed phenotype. The safety profile and immunogenicity of the 5x1010 vp dose level were supportive for further clinical development of Ad26.COV2.S as a protective vaccine against COVID-19. Additionally, the study examined the baseline demographics of 377, 25, and 394 participants in three groups (C1a, C1b, and C3). The vaccine was found to be safe and effective in inducing an immune response, as measured by the log geometric mean titers (GMTs) of SARS-CoV-2 binding antibodies in serum at baseline and at Day 29 post vaccination.\n",
            "\n",
            "\n",
            "The main topic of this text extract is the safety, reactogenicity, and immunogenicity of Ad26.COV2.S, a potential vaccine against COVID-19.\n",
            "Main_authors in the topic: \n",
            "\n",
            "\n",
            "S Kumar\n",
            "N Sharif\n",
            "KJ Alzahrani\n",
            "SN Ahmed\n",
            "SK Dey\n",
            "\n",
            "Main documents in the topic: \n",
            "\n",
            "\n",
            "1. Efficacy and safety of potential vaccine candidates against coronavirus disease 2019: A systematic review\n",
            "Authors: S Kumar\n",
            "\n",
            "2. Efficacy, immunogenicity and safety of COVID-19 vaccines: a systematic review and meta-analysis\n",
            "Authors: N Sharif, KJ Alzahrani, SN Ahmed, SK Dey\n",
            "\n",
            "3. Heterologous prime–boost strategies for COVID-19 vaccines\n",
            "Authors: B Sapkota, B Saud\n",
            "\n",
            "4. Safety, immunogenicity, and effectiveness of COVID-19 vaccines for pregnant persons: A protocol for systematic review and meta analysis\n",
            "Authors: A Ciapponi, M Berrueta, A Bardach\n",
            "\n",
            "5. An open, non-randomised, phase 1/2 trial on the safety, tolerability, and immunogenicity of single-dose vaccine “Sputnik Light” for prevention of coronavirus\n",
            "Authors: AI Tukhvatulin, IV Dolzhikova\n",
            "\n",
            "Main centers and institutions: \n",
            "\n",
            "\n",
            "AIIMS Deoghar\n",
            "Jahangirnagar University\n",
            "Nobel College\n",
            "Centro Cochrane Argentino-Instituto de Efectividad Clínica y Sanitaria (IECS-CONICET)\n",
            "N.F.Gamaleya National Research Center for Epidemiology and Microbiology\n",
            "MCH hospital\n",
            "\n",
            "--------------------------------------\n"
          ]
        }
      ],
      "execution_count": 23,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python-llama-cpp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "nteract": {
      "version": "0.28.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}